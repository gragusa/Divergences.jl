using MathOptInterface, Optimization, OptimizationMOI, OptimizationOptimJL, Ipopt
using ForwardDiff, DifferentiationInterface
using Divergences
using Statistics, LinearAlgebra
using Infiltrator
using StableRNGs

## -----
## High-performant
## -----
function randiv(; n = 100, m = 5, k = 1, Œ∏ = 0.0, œÅ = 0.9, CP = 20)
    ## Simulate
    ## y = xŒ∏ + w'Œ≥ + u
    ## x = zœÑ + w'Œæ + Œ∑
    ## where z ‚àº N(0, I‚Çò), w ‚àº N(0, I‚Çñ) 
    ## (Œ∑ ‚àº N(0, I), u ‚àº N(0, I)
    œÑ = fill(sqrt(CP / (m * n)), m)
    z = randn(n, m)  ## Instruments
    w = randn(n, k-1)  ## Exogenous
    Œ∑ = randn(n, 1)
    u = œÅ * Œ∑ + ‚àö(1 - œÅ^2) * randn(n, 1)
    x = z * œÑ + Œ∑
    y = x * Œ∏ + u
    return y, [x w], [z w]
end

y, x, z = randiv(k=1, CP=5)

p = (y = y,
    x = x,
    z = z,
    Y = similar(y),
    X = similar(x),
    Z = similar(z),
    ‚àÇG = Matrix{Float64}(undef, size(z, 2), size(x,2))
    );

function g(Œ∏, p)
    ## G‚Çô(Œ∏)
    (y, x, z, Y, X, Z, ‚àÇG) = p.data
    mul!(Y, x, Œ∏)
    broadcast!(-, Y, y, Y)
    broadcast!(*, Z, z, Y)
    return Z
end

function Dg(Œ∏, œÄ, p)
    (y, x, z, Y, X, Z, ‚àÇG) = p.data
    broadcast!(*, X, œÄ, x)
    mul!(‚àÇG, z', -X)
    return ‚àÇG
end

function DgŒª(Œ∏, Œª, p)
    (y, x, z, Y, X, Z, ‚àÇG) = p.data
    mul!(Y, z, Œª)
    broadcast!(*, X, -Y, x)
    broadcast!(/, X, X, n)
    return X
end

function DgŒª(Œ∏, Œª, œÄ, p)
    ## HœÄŒ∏
    (y, x, z, Y, X, Z, ‚àÇG) = p.data
    ‚àÇgŒª = DgŒª(Œ∏, Œª, p)
    broadcast!(*, ‚àÇgŒª, ‚àÇgŒª, œÄ)
    return ‚àÇgŒª
end

function DgŒª!(J, Œ∏, Œª, p)
    ‚àÇgŒª = DgŒª(Œ∏, Œª, p)
    copy!(J, vec(‚àÇgŒª))
end

function DgŒª!(J, Œ∏, Œª, œÄ, p)
    DgŒª(Œ∏, Œª, œÄ, p)
    copy!(J, p.‚àá)
end

function HgŒª(Œ∏, Œª, œÄ, p)
    n, k, m = size(p)
    zeros(k, k)
end

## --------------------------------------------------------------------- ##
## Optimization Problem
## --------------------------------------------------------------------- ##

const MOI = MathOptInterface

struct MDProblem <: MOI.AbstractNLPEvaluator
    div::Divergences.AbstractDivergence
    data
    backend
end

Base.size(md::MDProblem) = (size(p.x)..., size(p.z,2))
divergence(md::MDProblem) = md.div

function MOI.initialize(md::MDProblem, rf::Vector{Symbol})
    for feat in rf
        if !(feat in [:Grad, :Jac, :Hess])
            error("Unsupported feature $feat")
        end
    end
end

MOI.features_available(md::MDProblem) = [:Grad, :Jac, :Hess]

## --------------------------------------------------------------------- ##
## Objective function
## --------------------------------------------------------------------- ##
function MOI.eval_objective(md::MDProblem, u::Vector{Float64})
    ## Objective function
    ## ‚àë·µ¢ Œ≥(œÄ·µ¢)
    n, k, m = size(md)
    divergence(md)(view(u, 1:n))
end

function MOI.eval_objective_gradient(md::MDProblem, res, u)
    ## Gradient of the objective function
    ## ‚àáœÄ ‚àë·µ¢ Œ≥'(œÄ·µ¢)
    n, k, m = size(md)
    T = eltype(res)
    Divergences.gradient!(view(res, 1:n), divergence(md), view(u, 1:n))
    fill!(view(res, (n+1):(n+k)), zero(T))
end

## --------------------------------------------------------------------- ##
## Constraints
## --------------------------------------------------------------------- ##
function MOI.eval_constraint(md::MDProblem, res, u)
    ## Constraints
    ## ‚àë·µ¢ œÄ·µ¢ g(Œ∏·µ¢) = 0
    ## ‚àë·µ¢ œÄ·µ¢ = n
    n, k, m = size(md)
    Œ∏ = view(u, (n+1):(n+k))
    œÄ = view(u, 1:n)
    G = g(Œ∏, md)
    constraint!(res, œÄ, G)
end

function constraint!(Œº::AbstractVector{T}, w::AbstractVector, x::AbstractMatrix) where T
    fill!(Œº, zero(T))
    @inbounds for j in axes(x,2)
        for i in axes(x,1)
            Œº[j] += w[i]*x[i,j]
        end
    end
    Œº[end] = sum(w)
    return Œº
end

## --------------------------------------------------------------------- ##
## Constraints Jacobian
## --------------------------------------------------------------------- ##
function MOI.jacobian_structure(md::MDProblem)
    n, k, m = size(md)
    rowcol_of_dense(n+k,m+1)
end

function MOI.eval_constraint_jacobian(md::MDProblem, J, u)
    n, k, m = size(md)
    Œ∏ = view(u, (n+1):(n+k))
    œÄ = view(u, 1:n)
    G = g(Œ∏, md)
    #@. G = G/n
    ‚àÇg = Dg(Œ∏, œÄ, md)
    @. ‚àÇg = ‚àÇg
    assign_constraint_jacobian!(J, G, ‚àÇg)
end

"""
    assign_constraint_jacobian!(J, g, ‚àág)

Assigns the elements of the block matrix `X = [[G'; ones(1, n)]; [‚àág ; zeros(m, k)]]`.

# Arguments
- `J::Vector{Float64}`: A preallocated array of size `m * n + m * k`, where `m`, `n`, and `k` are the dimensions of `g` and `‚àág`.
- `g::AbstractMatrix{T}`: An `n √ó m` matrix.
- `‚àág::AbstractMatrix{T}`: An `m √ó k` matrix.
```
"""
function assign_constraint_jacobian!(J, gg, Dg)
    n, m = size(gg)
    k = size(Dg,2)
    for j in 1:n
        # Elements from gg'
        for i in 1:m
            J[(j-1)*(m+1) + i] = gg[j,i]
        end
        # Element from ones row
        J[j*(m+1)] = 1.0
    end
    # Next k columns (from Dg and zeros row)
    offset = n*(m+1)
    for j in 1:k
        # Elements from Dg
        for i in 1:m
            J[offset + (j-1)*(m+1) + i] = Dg[i,j]
        end
        # Element from 0
        J[offset + j*(m+1)] = 0.0
    end
    return J
end

## --------------------------------------------------------------------- ##
## Hessian of the Lagrangian of L(œÄ, Œ∏, Œª) = D(œÄ, p) + Œª'g(Œ∏)
## --------------------------------------------------------------------- ##
function MOI.hessian_lagrangian_structure(md::MDProblem)
    n, k, m = size(md)
    hele = Int(n + n*k + k*(k+1)√∑2)
    rows = Array{Int64}(undef, hele)
    cols = Array{Int64}(undef, hele)
    ## Diagonal Elements
    for j = 1:n
        rows[j] = j
        cols[j] = j
    end
    idx = n+1
    ## Off-diagonal elements
    for j = 1:k
        for s = 1:n
            rows[idx] = n + j
            cols[idx] = s
            idx += 1
        end
    end
    ## For linear problem this is not needed
    for j = 1:k
        for s = 1:j
            rows[idx] = n + j
            cols[idx] = n + s
            idx += 1
        end
    end
    [(r, c) for (r, c) in zip(rows,cols)]
end

function MOI.eval_hessian_lagrangian(md::MDProblem, hess, u, œÉ, Œª)
    n, k, m = size(md)
    œÄ = view(u, 1:n)
    Œ∏ = view(u, (n+1):(n+k))
    if œÉ==0
        @inbounds for j=1:n
            hess[j] = 0.0
        end
    else
        hv = view(hess, 1:n)
        Divergences.hessian!(hv, divergence(md), œÄ)
        hv .= hv.*œÉ
    end
    Œªv = view(Œª, 1:m)
    DgŒª!(view(hess, n+1:n+n*k), Œ∏, Œªv, md)
    ## For linear problem this is not needed
    copy_lower_triangular!(view(hess, n+n*k+1:n+n*k+(k*(k+1)√∑2)), HgŒª(Œ∏, Œª, œÄ, md))
end

## --------------------------------------------------------------------- ##
## Problem with fixed theta
## --------------------------------------------------------------------- ##

struct SMDProblem <: MOI.AbstractNLPEvaluator
    div::Divergences.AbstractDivergence
    G::Matrix{Float64}
    data
    backend
end

divergence(md::SMDProblem) = md.div
momfun(md::SMDProblem) = md.G

function MOI.initialize(md::SMDProblem, rf::Vector{Symbol})
    for feat in rf
        if !(feat in [:Grad, :Jac, :Hess])
            error("Unsupported feature $feat")
        end
    end
end

MOI.features_available(md::SMDProblem) = [:Grad, :Jac, :Hess]

function MOI.eval_objective(md::SMDProblem, u::Vector{Float64})
    divergence(md)(u)
end

function MOI.eval_objective_gradient(md::SMDProblem, res, u)
    n, k, m = size(md)
    T = eltype(res)
    Divergences.gradient!(res, divergence(md), u)
end

## --------------------------------------------------------------------- ##
## Constraints
## --------------------------------------------------------------------- ##
function MOI.eval_constraint(md::SMDProblem, res, u)
    œÄ = u
    G = md.G
    constraint!(res, œÄ, G)
end

function constraint!(Œº::AbstractVector{T}, w::AbstractVector, x::AbstractMatrix) where T
    fill!(Œº, zero(T))
    @inbounds for j in axes(x,2)
        for i in axes(x,1)
            Œº[j] += w[i]*x[i,j]
        end
    end
    Œº[end] = sum(w)
    return Œº
end

## --------------------------------------------------------------------- ##
## Constraints Jacobian
## --------------------------------------------------------------------- ##
function MOI.jacobian_structure(md::SMDProblem)
    n, k, m = size(md)
    rowcol_of_dense(n,m+1)
end

function MOI.eval_constraint_jacobian(md::MDProblem, J, u)
    œÄ = u
    G = md.G
    #@. G = G/n
    assign_constraint_jacobian!(J, G)
end

"""
    assign_constraint_jacobian!(J, g)

Assigns the elements of the block matrix `X = G'`.

# Arguments
- `J::Vector{Float64}`: A preallocated array of size `m * n + m * k`, where `m`, `n`, and `k` are the dimensions of `g` and `‚àág`.
- `g::AbstractMatrix{T}`: An `n √ó m` matrix.
```
"""
function assign_constraint_jacobian!(J, gg)
    n, m = size(gg)
    k = size(Dg,2)
    for j in 1:n
        # Elements from gg'
        for i in 1:m
            J[(j-1)*(m+1) + i] = gg[j,i]
        end
        # Element from ones row
        J[j*(m+1)] = 1.0
    end
    return J
end

## --------------------------------------------------------------------- ##
## Hessian of the Lagrangian of L(œÄ, Œ∏, Œª) = D(œÄ, p) + Œª'g(Œ∏)
## --------------------------------------------------------------------- ##
function MOI.hessian_lagrangian_structure(md::SMDProblem)
    rows = Array{Int64}(undef, n)
    cols = Array{Int64}(undef, n)
    ## Diagonal Elements
    for j = 1:n
        rows[j] = j
        cols[j] = j
    end
    [(r, c) for (r, c) in zip(rows,cols)]
end

function MOI.eval_hessian_lagrangian(md::SMDProblem, hess, u, œÉ, Œª)
    œÄ = u
    if œÉ==0
        @inbounds for j=1:n
            hess[j] = 0.0
        end
    else
        hv = view(hess, 1:n)
        Divergences.hessian!(hv, divergence(md), œÄ)
        hv .= hv.*œÉ
    end
end





## --------------------------------------------------------------------- ##
## Problem
## --------------------------------------------------------------------- ##

‚Ñ≥ùíü = FullyModifiedDivergence(ReverseKullbackLeibler(), 0.1, 1.2)
mdprob = MDProblem(‚Ñ≥ùíü, p, nothing)

model = Ipopt.Optimizer()
œÄ = MOI.add_variables(model, n)
MOI.add_constraint.(model, œÄ, MOI.GreaterThan(0.0))
Œ∏ = MOI.add_variables(model, k)
MOI.add_constraint.(model, Œ∏, MOI.GreaterThan(-10.0))
MOI.add_constraint.(model, Œ∏, MOI.LessThan(+10.0))
for i ‚àà 1:n
    MOI.set(model, MOI.VariablePrimalStart(), œÄ[i], 1.0)
end
for i ‚àà 1:k
    MOI.set(model, MOI.VariablePrimalStart(), Œ∏[i], 0.0)
end
lb = [zeros(m); n]
ub = [zeros(m); n]
MOI.set(model, MOI.ObjectiveSense(), MOI.MIN_SENSE)

model_el = deepcopy(model)
model_md = deepcopy(model)

block_data = MOI.NLPBlockData(MOI.NLPBoundsPair.(lb, ub), mdprob, true)
MOI.set(model_md, MOI.NLPBlock(), block_data)
for i ‚àà 1:k
    MOI.set(model_md, MOI.VariablePrimalStart(), Œ∏[i], -0.01)
end

mdprob = MDProblem(ReverseKullbackLeibler(), p, nothing)
block_data = MOI.NLPBlockData(MOI.NLPBoundsPair.(lb, ub), mdprob, true)
MOI.set(model_el, MOI.NLPBlock(), block_data)




model.options["derivative_test"] = "none"
model.options["derivative_test_print_all"] = "no"

model.options["print_level"] = 4

MOI.optimize!(model)
MOI.get(model, MOI.TerminationStatus())
MOI.get(model, MOI.DualStatus())
MOI.get(model, MOI.PrimalStatus())

MOI.get(model, MOI.SolveTimeSec())
MOI.get(model, MOI.BarrierIterations())

xstar = MOI.get(model, MOI.VariablePrimal(), Œ∏)

function lagrangian(md::MDProblem, u, œÉ, Œª)
    n, k, m = size(md)
    œÄ = u[1:n]
    Œ∏ = u[(n+1):(n+k)]
    œÉ.*divergence(md)(œÄ) +mean(œÄ.*g(Œ∏, md)*Œª)
end


using Statistics
p = [0.45793379249066035, 4.999416892014921, 9.182989399836064, 3.6958463315972025, 6.220383439227501, 0.019436036309187443, 2.063484686999562, 10.894774879314305, 8.25546846552471, 4.029010019680072, -2.975818044182361, 1.4669020891138018]

lagrangian(mdprob, p, 1.0, [1.5, 0.0])

H0 = ForwardDiff.hessian(x -> lagrangian(mdprob, x, 1.5, [1.5, 0.0]), p);
H = zeros(34)
MOI.eval_hessian_lagrangian(mdprob, H, p, 1.5, [1.5, 0.0])

H0 = ForwardDiff.hessian(x -> lagrangian(mdprob, x, 0.0, [1.5, 0]), p);
MOI.eval_hessian_lagrangian(mdprob, H, p, 0.0, [1.5, 0])

## --------------------------------------------------------------------- ##
## Simple MC
## --------------------------------------------------------------------- ##

Œ≤_el = Matrix{Float64}(undef, 1000, 3)
f_el = zeros(1000)
Œ≤_md = Matrix{Float64}(undef, 1000, 3)
f_md = zeros(1000)
for j in 1:1000
    y, x, z = randiv(k=1, CP=5)
    p.y .= y
    p.x .= x
    p.z .= z
    MOI.optimize!(model_el)
    MOI.optimize!(model_md)
    Œ≤_el[j,:] .= MOI.get(model_el, MOI.VariablePrimal(), Œ∏)
    Œ≤_md[j,:] .= MOI.get(model_md, MOI.VariablePrimal(), Œ∏)
    f_el[j] = model_el.inner.status
    f_md[j] = model_md.inner.status
end

using StatsPlots

StatsPlots.density(Œ≤)
StatsPlots.histogram(Œ≤, nbins = 80)



## --------------------------------------------------------------------- ##
## Utilities
## --------------------------------------------------------------------- ##


# function assign_matrix(J, gg, Dg)
#     n, m = size(gg)
#     k = size(Dg,2)
#     R = [ [gg'; ones(1, n)] [Dg; zeros(1,k)]]
#     J .= vec(R)
# end


using SparseArrays

function rowcol_of_sparse(g::SparseMatrixCSC; offset_row = 0, offset_col = 0)
    rows = rowvals(g)
    vals = nonzeros(g)
    m, n = size(g)
    tup = Tuple{Int64, Int64}[]
    for j ‚àà 1:n
        for i ‚àà nzrange(g, j)
            push!(tup, (rows[i]+offset_row, j+offset_col))
        end
    end
    return tup
end


function weighted_sum(G, w)
    n, m = size(G)
    res = zeros(eltype(G), m)
    @inbounds for j in axes(G,2)
        for i in axes(G,1)
            res[j] += w[i]*G[i,j]
        end
    end
    return res
end

function weighted_sum2(G, w)
    @inbounds vec(sum(w.*G, dims=1))
end


"""
    rowcol_of_dense(g::AbstractMatrix; offset_row = 0, offset_col = 0)

Returns a tuple of row and column indices for all elements in a dense matrix `g`, with optional offsets for rows and columns.

# Arguments
- `g::AbstractMatrix`: The input dense matrix.
- `offset_row::Int` (default: 0): An offset to be added to each row index.
- `offset_col::Int` (default: 0): An offset to be added to each column index.

# Returns
A vector of tuples `(row, col)` representing the indices of all elements in the dense matrix.

# Example
```julia
g = [1 2; 3 4]
rowcol_of_dense(g)  # [(1, 1), (2, 1), (1, 2), (2, 2)]
```
"""
function rowcol_of_dense(n ,m; offset_row = 0, offset_col = 0)
    tup = Tuple{Int64, Int64}[]  # Initialize an empty vector of tuples
    @inbounds for j ‚àà 1:n
        for i ‚àà 1:m
            push!(tup, (i + offset_row, j + offset_col))
        end
    end
    return tup
end




function copy_lower_triangular!(x::AbstractVector{T}, A::Matrix{T}) where T
    @assert issquare(A)
    n = size(A, 1)
    len = (n * (n + 1)) √∑ 2  # Length of output vector
    @assert len == (n * (n + 1)) √∑ 2
    idx = 1
    @inbounds for j in 1:n
        for i in j:n
            x[idx] = A[i, j]
            idx += 1
        end
    end
    return x
end

function copy_lower_triangular!(x::AbstractVector{T}, A::Vector{T}) where T
    n = length(A)
    @assert n == 1 "`copy_lower_triangular!` for vector make sense only for singleton vector"
    @assert 1 == (n * (n + 1)) √∑ 2 "The dimension of the dest vector is wrong as it should be $(n*(n+1))//2"
    x .= A
    return x
end



abstract type SmootherType end

struct Truncated <: SmootherType end
struct Bartlett <: SmootherType end

@inline weight(::Truncated, s, St) = 1.0
@inline weight(::Bartlett, s, St) = 1.0 - s/St

# Base version
function smooter_base(tt::T, G::Matrix, Œæ::Integer) where {T<:SmootherType}
    N, M = size(G)
    nG = zeros(N, M)
    St = (2.0 * Œæ + 1.0) / 2.0
    for m = 1:M
        for t = 1:N
            low = max((t - N), -Œæ)
            high = min(t - 1, Œæ)
            for s = low:high
                Œ∫ = weight(tt, s, St)
                @inbounds nG[t, m] += Œ∫ * G[t-s, m]
            end
        end
    end
    return (nG ./ (2 * Œæ + 1))
end

function smoother(tt::Truncated, G::Matrix{T}, Œæ::Integer) where {T}
    N, M = size(G)
    nG   = Matrix{T}(undef, N, M)
    smoother!(tt, nG, G, Œæ)
end

function smoother!(tt::Truncated, dest, G::Matrix{T}, Œæ::Integer) where {T}
    N, M = size(G)
    denom = 2Œæ + 1  # normalization
    Threads.@threads for m in 1:M
        for t in 1:N
            low  = max(t - N, -Œæ)
            high = min(t - 1,  Œæ)
            acc  = zero(T)
            @inbounds for s in low:high
                Œ∫ = weight(tt, s, Œæ)
                acc += G[t - s, m]
            end
            dest[t, m] = acc / denom
        end
    end
    return dest
end

# optprob = OptimizationFunction(divergence, Optimization.AutoForwardDiff(), cons = cons)
# prob = OptimizationProblem(optprob, x0, _p,
#                            lcons = repeat([0.], 2),
#                            ucons = repeat([0.], 2),
#                            lb = [repeat([0], 100); -Inf],
#                            ub = [repeat([+Inf], 100); +Inf])

# solver = OptimizationMOI.MOI.OptimizerWithAttributes(Ipopt.Optimizer, "print_level" => 0)

# solve(prob, solver)
